{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "New TF model building and traing routine! \n",
    "Uses:\n",
    "    2mer negative controls\n",
    "    regresion off actual counts\n",
    "    better poisson counting\n",
    "    gradient lassoing thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/kal/TF_models/bin/sequence.py:275: RuntimeWarning: divide by zero encountered in log\n",
      "  self.seq = helper.softmax(np.log(dist))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model without Bias layer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/kal/TF_models/bin/')\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2' # Must be before importing keras!\n",
    "import tf_memory_limit\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, LearningRateScheduler\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Input, Lambda, Dense, Conv1D, Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt \n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from scipy.integrate import trapz\n",
    "from tqdm import tqdm\n",
    "import ucscgenome\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import sequence\n",
    "import train_TFmodel\n",
    "import eval_TFmodel\n",
    "import ctcfgen\n",
    "import seq_only_gen\n",
    "import train_seq_regression_convnet\n",
    "pwm = eval_TFmodel.TFmodel('/home/kal/TF_models/seq_only/seq_classifier/pwm_frozen/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "out_path = os.path.join('/home/kal/TF_models/seq_only/count_regression/9_channel_CTCF')\n",
    "os.makedirs(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make some paths\n",
    "bed_path = '/home/kal/TF_models/data/count_regression/ctcf_regions_9_log.bed'\n",
    "columns='chr start end name score nucs nlog1 nlog2 nlog3 nlog4 nlog5 nlog6 nlog7 nlog8 nlog9'\n",
    "score_columns ='nlog1 nlog2 nlog3 nlog4 nlog5 nlog6 nlog7 nlog8 nlog9'.split()\n",
    "peaks = pd.read_table(bed_path, header=None)\n",
    "peaks.columns = columns.split()\n",
    "\n",
    "prediction_window = 256\n",
    "half_window = prediction_window // 2\n",
    "num_training_examples = sum(peaks.chr != 'chr8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def native_gen(mode='train', once=False):\n",
    "    \"\"\"Generate a positive seqeunce sample.\"\"\"\n",
    "    done = False\n",
    "    if mode == 'test':\n",
    "        indices = np.asarray(peaks[peaks.chr == 'chr8'].index.values)\n",
    "        indices = [x for x in indices if x%2 == 0]\n",
    "    elif mode =='val':\n",
    "        indices = np.asarray(peaks[peaks.chr == 'chr8'].index.values)\n",
    "        indices = [x for x in indices if x%2 == 1]\n",
    "    else:\n",
    "        indices = np.asarray(peaks[peaks.chr != 'chr8'].index.values)\n",
    "    while not done:\n",
    "        np.random.shuffle(indices)\n",
    "        for idx in indices:\n",
    "            if len(score_columns) == 1:\n",
    "                yield peaks.get_value(idx, 'nucs'), peaks.get_value(idx, score_columns)\n",
    "            else:\n",
    "                scores=list()\n",
    "                for c in score_columns:\n",
    "                    scores.append(peaks.get_value(idx, c))\n",
    "                yield peaks.get_value(idx, 'nucs'), np.asarray(scores)\n",
    "            done = once\n",
    "            \n",
    "def scrambled_gen(scrambled, mode='train'):\n",
    "        posgen = native_gen(mode=mode)\n",
    "        if prediction_window % scrambled != 0:\n",
    "            print(str(scrambled) + 'mers do not evenly divide the sequence.')\n",
    "            scrambled = 1\n",
    "        for p, q in posgen:\n",
    "            p = np.asarray([base for base in p])\n",
    "            p = p.reshape((-1,scrambled))\n",
    "            np.random.shuffle(p)\n",
    "            p = p.reshape([-1])\n",
    "            yield ''.join(p) \n",
    "            \n",
    "            \n",
    "def pair_gen(mode='train', once=False, batch_size=32):\n",
    "    \"\"\"Generate batched of paired samples.\"\"\"\n",
    "    p = native_gen(mode=mode, once=once)\n",
    "    n = scrambled_gen(2, mode=mode)\n",
    "    while True:\n",
    "        pos_seqs = list()\n",
    "        neg_seqs = list()\n",
    "        scores = list()\n",
    "        for i in range(batch_size // 2):\n",
    "            pos_seq, score = next(p)\n",
    "            neg_seq = next(n)\n",
    "            pos_seqs.append(sequence.encode_to_onehot(pos_seq))\n",
    "            neg_seqs.append(sequence.encode_to_onehot(neg_seq))\n",
    "            scores.append(score)\n",
    "        labels = np.append(np.asarray(scores), np.zeros((32 // 2, len(scores[0]))), axis=0)\n",
    "        yield np.asarray(pos_seqs + neg_seqs), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Define some params\n",
    "batch_size = 32\n",
    "drop_rate = 0.1\n",
    "conv_string='32.3_32.32_32.3_16.3'\n",
    "conv_list = [[int(x) for x in cell.split('.')] for cell in conv_string.split('_')]\n",
    "num_outputs = len(score_columns)\n",
    "# Get the time-tag for the model.\n",
    "timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "# make a file system\n",
    "weights_path = os.path.join(out_path, 'intermediate_weights')\n",
    "os.makedirs(weights_path)\n",
    "history_path = os.path.join(out_path, 'history')\n",
    "os.makedirs(history_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convolutions used: [[32, 3], [32, 32], [32, 3], [16, 3]] [neurons, filter]\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "# A one how input with reverse complement -- > series of convolutions --> smoothing --> maximum over directions --> bias --> activation\n",
    "# Input *is* one hot encoded - and type np.uint8.\n",
    "input = Input(batch_shape=(batch_size, prediction_window, 4))\n",
    "# add reverse complement so the model only has to learn one direciton\n",
    "add_RC_to_batch = Lambda(lambda x: K.concatenate([x, x[:, ::-1, ::-1]], axis=0), output_shape=lambda s: (2 * s[0], s[1], s[2]))  \n",
    "# output an acitvaiton at each base form convolutions of convolutions\n",
    "per_base_score = train_TFmodel.BasicConv(prediction_window, conv_list, final_activation=None, num_outputs=num_outputs, drop_rate=drop_rate) \n",
    "# Take wide-window convolution scan with fixed wieghts to smooth out peaks.\n",
    "wide_scan = Conv1D(num_outputs, 50, use_bias=False, kernel_initializer='ones', trainable=False, name='wide_scan', padding='valid')\n",
    "# Get the forward/reverse sequence maximum. \n",
    "max_by_direction = Lambda(lambda x: K.maximum(K.max(x[:x.shape[0]//2, :, :], axis=1), K.max(x[x.shape[0]//2:, ::-1, :], axis=1)), name='stackmax', output_shape=lambda s: (s[0] // 2, num_outputs))\n",
    "# Add a custom bias layer\n",
    "final_bias = train_TFmodel.Bias(num_outputs, name='bias')\n",
    "\n",
    "# build the model\n",
    "predictions = final_bias(max_by_direction(wide_scan(per_base_score(add_RC_to_batch(input)))))\n",
    "model = Model(inputs=[input], outputs=[predictions])\n",
    "# save a graph of the model configuration\n",
    "#plot_model(model, to_file=os.path.join(out_path, timestr + '_' + conv_string + '_model.png'), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verb=0\n",
    "loss_func='mean_squared_error'\n",
    "num_epochs=3\n",
    "\n",
    "# create optimizers for the three learning phases with learning rate 1/10th of previous at each step\n",
    "optimizer_1 = Adam(beta_1=0.95, lr=0.0005, epsilon=.1)\n",
    "optimizer_2 = Adam(beta_1=0.95, lr=0.00005, epsilon=.001)\n",
    "optimizer_3 = Adam(beta_1=0.95, lr=0.000005, epsilon=.00001)\n",
    "\n",
    "# Create a callback to save the model weights.\n",
    "checkpath = os.path.join(weights_path, '_'.join([timestr, 'weights_1_{epoch:02d}_{val_loss:.2f}.hdf5']))\n",
    "checkpointer = ModelCheckpoint(checkpath, verbose=verb, monitor='val_loss', mode='min')\n",
    "callbacks_list = [checkpointer]\n",
    "\n",
    "#create data generators\n",
    "traingen=pair_gen(mode='train')\n",
    "valgen=pair_gen(mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14522 batches\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "num_batches = len(peaks[peaks.chr != 'chr8']) // batch_size\n",
    "print(str(num_batches) + ' batches')\n",
    "\n",
    "# go for 250 'epochs' in each of the three training stages\n",
    "# each 'epoch' is actually only 1/50 of the data -- so 5 real epochs\n",
    "\n",
    "# compile and run the first iteration of the training params and model\n",
    "model.compile(loss=loss_func, optimizer=optimizer_1, metrics=['mse'])\n",
    "\n",
    "History_1 = model.fit_generator(traingen, num_batches // 50, epochs=num_epochs*50 // 3, validation_data=valgen, validation_steps=20, callbacks=callbacks_list, verbose=verb)\n",
    "\n",
    "# compile and train the second iteration\n",
    "checkpath = os.path.join(weights_path, '_'.join([timestr, 'weights_2_{epoch:02d}_{val_loss:.2f}.hdf5']))\n",
    "checkpointer = ModelCheckpoint(checkpath, verbose=verb, monitor='val_loss', mode='max')\n",
    "callbacks_list = [checkpointer]\n",
    "model.compile(loss=loss_func, optimizer=optimizer_2, metrics=['mse'])\n",
    "History_2 = model.fit_generator(traingen, num_batches // 50, epochs=num_epochs*50 // 3, validation_data=valgen, validation_steps=20, callbacks=callbacks_list, verbose=verb)\n",
    "\n",
    "# compile and train the third iteration\n",
    "checkpath = os.path.join(weights_path, '_'.join([timestr, 'weights_3_{epoch:02d}_{val_loss:.2f}.hdf5']))\n",
    "checkpointer = ModelCheckpoint(checkpath, verbose=verb, monitor='val_loss', mode='min')\n",
    "callbacks_list = [checkpointer]\n",
    "model.compile(loss=loss_func, optimizer=optimizer_3, metrics=['mse'])\n",
    "History_3 = model.fit_generator(traingen, num_batches // 50, epochs=num_epochs*50 // 3, validation_data=valgen, validation_steps=20, callbacks=callbacks_list, verbose=verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#do this in bash\n",
    "print('Training model')\n",
    "train_seq_regression_convnet.make_model(out_path, '32.3_32.32_16.3_8.3', gen_path, loss_func='mean_squared_error', num_epochs=9, verb=0)\n",
    "ml_model = eval_TFmodel.TFmodel(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get training loss stuff\n",
    "\n",
    "out_path='/home/kal/TF_models/seq_only/count_regression/test2_CTCF/'\n",
    "ml_model = eval_TFmodel.TFmodel(out_path)\n",
    "\n",
    "for file in os.listdir(os.path.join(out_path, 'history')):\n",
    "    file = os.path.join(os.path.join(out_path, 'history'), file)\n",
    "    # find the history pickles\n",
    "    if file.endswith('1.pk1'):\n",
    "        with open(file, 'rb') as infile:\n",
    "            h1 = pickle.load(infile)\n",
    "    elif file.endswith('2.pk1'):\n",
    "        with open(file, 'rb') as infile:\n",
    "            h2 = pickle.load(infile)\n",
    "    elif file.endswith('3.pk1'):\n",
    "        with open(file, 'rb') as infile:\n",
    "            h3 = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Summarize history for accuracy\n",
    "plt.plot(eval_TFmodel.group_stats('loss', h1, h2, h3))\n",
    "plt.plot(eval_TFmodel.group_stats('val_loss', h1, h2, h3))\n",
    "plt.title('Training and Validation Loss for CTCF model')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# predict on all atac sequences from k562 and make a p-r and p-r gain curve\n",
    "npreds = dict()\n",
    "bed_path = '/home/kal/TF_models/data/K562_atac_peaks/final_atac.bed'\n",
    "peaks = pd.read_table(bed_path, header=None)\n",
    "peaks = peaks.sample(1000)\n",
    "peaks.columns = 'chr start end ctcf_label . . '.split()\n",
    "peaks = peaks[peaks['chr']!='chrM']\n",
    "print(len(peaks))\n",
    "pwm_preds = pwm.predict_bed(peaks)\n",
    "true_labels = peaks['ctcf_label']\n",
    "ml_preds = ml_model.predict_bed(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#p-r curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('P-R Curve for CTCF Binding in K562 ATAC Regions')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "pwm_p, pwm_r, pwm_t = precision_recall_curve(peaks['ctcf_label'], pwm_preds, pos_label=1)\n",
    "plt.plot(pwm_r, pwm_p, label='PWM Model')\n",
    "p, r, t = precision_recall_curve(peaks['ctcf_label'], ml_preds, pos_label=1)\n",
    "plt.plot(r, p, label='ML Model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "print('AOC: ' + str(- trapz(p,r)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
